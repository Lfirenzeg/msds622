id,model,objective,variation,controls,metrics,result,conclusion,recommend
DT-1,Decision Tree,Improve minority recall with class weights and pruning.,Grid over cp/minsplit/maxdepth; weights=TRUE,Same features/split/CV; no SMOTE.,"PR-AUC (primary), Recall@Top10%, ROC-AUC","Test PR-AUC=0.4628, ROC-AUC=0.7653, Recall@10%=0.3869",Weighted and pruned tree outperformed baseline on PR-AUC/Recall@10%.,Adopt cost-sensitive settings with selected cp/minsplit/maxdepth.
DT-2,Decision Tree,"Test engineered features (recent_contact, intensity, interactions) with class weights.",Features with interactions; grid over cp/minsplit/maxdepth; weights=TRUE,Same split/CV; duration excluded; na.roughfix for tree.,"PR-AUC (primary), Recall@Top10%, ROC-AUC","Test PR-AUC=0.4628, ROC-AUC=0.7653, Recall@10%=0.3869",Feature engineering held parity and slightly improved ranking consistency.,Keep engineered features; retain weights and pruning.
RF-1,Random Forest,Establish a fast ensemble baseline.,"num.trees=500, mtry=5, no depth cap, class weights",Same features/split; na.roughfix; class weights for imbalance.,"PR-AUC (primary), Recall@Top10%, ROC-AUC","Test PR-AUC=0.3295, ROC-AUC=0.7345, Recall@10%=0.3502",Underperforms against DT-1/DT-2,Readjust mtry for RF-2.
RF-2,Random Forest (ranger),"Improve ranking (PR-AUC, Recall@Top10%) by faster RF + tuning mtry and shallow depth control.",3-fold CV; num.trees=500; mtry=5; max.depth=12; min.node.size=1; class weights; ranger engine,Same engineered feature frame as DT-2 (with level compression); same train/test split and metrics.,"Primary: PR-AUC; Secondary: Recall@Top10%, ROC-AUC","CV: PR-AUC=0.452, ROC-AUC=0.798, Recall@10%=0.449 | Test: PR-AUC=0.462, ROC-AUC=0.804, Recall@10%=0.453",Tuned ranger RF substantially outperforms RF-1 and improves top-decile recall vs DT while matching PR-AUC. Depth cap helped reduce noise; larger mtry values did not help further.,"Prefer RF-2 over RF-1. Compare RF-2 against the pruned DT for deploymentâ€”RF-2 gives higher Recall@Top10% (more conversions at fixed outreach), with acceptable interpretability via feature importance."
AB-1,AdaBoost (fastAdaboost stumps),Fast boosting baseline with weak learners (depth=1).,"maxdepth=1, mfinal=150, coeflearn='Zhu'; 3-fold CV; no explicit class weights.",Same features/split; na.roughfix to handle NAs consistently.,"PR-AUC (primary), Recall@Top10%, ROC-AUC","Test PR-AUC=0.3486, ROC-AUC=0.7573, Recall@10%=0.3696",Stumps underfit; lower PR-AUC and recall vs AB2,Use as baseline only; prefer AB2.
AB-2,AdaBoost (adabag + rpart shallow),Increase model capacity with shallow weak learners while maintaining generalization.,"maxdepth=3, mfinal=150, coeflearn='Zhu'; 3-fold CV; no explicit class weights.",Same features/split; na.roughfix.,"PR-AUC (primary), Recall@Top10%, ROC-AUC","Test PR-AUC=0.4529, ROC-AUC=0.7910, Recall@10%=0.4429",Depth=3 boosted PR-AUC and Recall@10; generalization test better than CV.,Keep AB2 as a co-finalist with RF2.
